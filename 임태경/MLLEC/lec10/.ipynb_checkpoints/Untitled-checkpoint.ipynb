{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid보다 ReLU가 더 좋아\n",
    "\n",
    "input layer - hidden layer - output layer 로 이루어져있다.\n",
    "\n",
    "with tf.name_scope(\"Layer1\") as scope:\n",
    "    L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "tf.name_scope로 layer에 이름부여를 한후 tensorboard로 시각화할수있다.\n",
    "\n",
    "### tensorboard cost & accuracy\n",
    "\n",
    "sigmoid함수를 사용할때 layer수가 너무 많아도 문제가 생긴다.\n",
    "\n",
    "backpropagation을 할때 f에서 출발한후 2,3번째 layer까진 문제가 없지만 많은수의 layer을 통과하며 맨앞의 layer에 도달했을때\n",
    "\n",
    "0에 아주근사한 값을 가지게되는데 이에 의미는 x1,x2 의 값이 최종출력에 영향을 끼치지 못한다는뜻이다.\n",
    "\n",
    "= vanishing gradient = 경사도가 사라져버린다 = 학습이되지않는다.\n",
    "\n",
    "ReLU -> 값이 0보다 작을경우 activate하지않고 0보다 클경우 비례해서 값을 준다.(Rectified Linear Unit)\n",
    "\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1) --> neural network에서 더이상 sigmoid함수를 사용하면 좋지못함\n",
    "\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "layer의 마지막단은 sigmoid함수를 사용하여야한다 -> 0~1사이의값을 가져야하기때문\n",
    "\n",
    "# Weight 초기화 잘해보자\n",
    "\n",
    "지난강의 vanishing gradient를 해결하기위해\n",
    "\n",
    "첫번째로 relu함수를 이용하는것이고 두번째는 초기값을 잘주는것.\n",
    "\n",
    "초기값을 0으로주게되면 학습이 전혀되지않는다.\n",
    "\n",
    "RBM 머신으로 w를 구했었음 -> 입력과 출력의 갯수로써 w값을 구하는 공식이 만들어짐\n",
    "\n",
    "W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in/2)\n",
    "\n",
    "# Dropout과 앙상블\n",
    "\n",
    "### overfitting(영역을 심각하게 구부려서 100%로 나누는것)\n",
    "\n",
    "-> 모델이 training data 에만 맞춰져있어 다른 test 데이터에는 엉뚱한 수치가 나옴\n",
    "\n",
    "deep하게 될수록 hyperplan이 너무 완벽하게 짜여짐\n",
    "\n",
    "### dropout\n",
    "\n",
    "랜덤하게 뉴런들을 죽이고 남은 뉴런들로 학습을 한뒤 마지막에는 모든 뉴런을 가지고 테스트를하는것.\n",
    "\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "\n",
    "TRAIN: #학습할시\n",
    "\n",
    "    sess.run(optimizer, feed_dict={X:batch_xs, Y:batch_ys, dropout_rate:0.7}) #dropout 시킬확률\n",
    "\n",
    "EVALUATION: #test데이터를 돌릴시\n",
    "    print \"Accuracy:\", accuracy.eval({X:mnist.test.images, Y:mnist.test.labels, dropout_rate:1})\n",
    "    \n",
    "    -> 실제데이터를 돌릴때 dropout_rate를 1로 설정해 모든 뉴런이 참여하도록한다.\n",
    "    \n",
    "### Ensemble\n",
    "\n",
    "여러개의 learning 모델을 만든후 결합시켜 결과를 물어보는것. 여러개의 learning모델은 초기값이 다르므로 조금씩 성능이 차이가난다.\n",
    "\n",
    "모든 learning모델을 앙상블을 한 결과는 일반 결과보다 2~5%의 성능차이를보인다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
